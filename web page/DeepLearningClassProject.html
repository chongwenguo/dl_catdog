<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Class Project
    | Georgia Tech | Fall 2018: CS 4803 / 7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Le styles -->
  <link href="css/bootstrap.css" rel="stylesheet">
  <style>
    body {
      padding-top: 60px;
      /* 60px to make the container go all the way to the bottom of the topbar */
    }

    .vis {
      color: #3366CC;
    }

    .data {
      color: #FF9900;
    }


  </style>

  <link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
  <div class="container">
    <div class="page-header">

      <!-- Title and Name -->
      <h1>Cats and Dogs</h1>
      <span style="font-size: 20px; line-height: 1.5em;"><strong>Chongwen Guo, Zhuoran Yu</strong></span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Spring 2019 CS 4803 / 7643 Deep Learning: Class
        Project</span><br>
      <span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
      <hr>

      <!-- Goal -->
      <!-- <h2>Abstract</h2>

One or two sentences on the motivation behind the problem you are solving. One or two sentences describing the approach you took. One or two sentences on the main result you obtained.
<br><br> -->
      <!-- figure -->
      <!-- <h2>Teaser figure</h2>
A figure that conveys the main idea behind the project or the main application being addressed. (This one is from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">AlexNet</a>.)
<br><br> -->
      <!-- Main Illustrative Figure -->
      <!-- <div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/alexnet.png">
</div> -->
      <div style="text-align: center;">
        <img style="height: 230px;" alt="" src="images/sample-dataset.jpg">
        <br>Figure 1. The Oxford-IIIT Pet Dataset
      </div>
      <br>
      <!-- Introduction -->
      <h2>Introduction / Background / Motivation</h2>

      
      <h4>Introduction</h4>
      This project aims to examine whether modern computer vision techniques work well in classification problems where
      different classes of images have relatively high similarities. The dataset we chose for this project is 
      the Oxford-IIIT Pet Dataset published by VGG group. 
      We are interested in three classification problems: binary classification of cats and dogs, cat breeds classification,
      and dog breeds classification. 
      The project consists of three major stages: build models from scratch, fine-tune pre-trained models and data augmentation with GAN.
      The Oxford-IIIT Pet Dataset contains 37 categories of pets with roughly 200 images for each class. The images have
      a large variations in scale, pose and lighting. Due to high inter class variability, low intra class variability
      and high pose variability, it is not easy to train and obtain a high accarucy breed-classifier, especially when
      the data volume is limited.
      We would like to use GAN to generate several fake cats and dog images. With generated images, we would like
      to see if they can improve accuracy of classifiers if used as data augmentation and if the classifier can
      correctly classify them.
      <br><br>

      <br><br>
      <div class="container" style='display: flex;'>

        <div style='flex: 1;text-align: center'>
          <img style="height: 380px;" alt="" src="images/breed_count.jpg">
          <br>Figure 2: Dataset Statistics

        </div>
        <div style='flex: 1;text-align: left'>
          <img style="height: 120px;" alt="" src="images/low-inter-class-variability.jpg">
          <br>Figure 3: Example of high intra class variability with two American Pit Bull Terriers and two Maine coons

          <br><img style="height: 100px;" alt="" src="images/high-intra-class-variability.jpg">
          <br>Figure 4: Example of low inter class variability with Beagle, Saint Bernard and British Shorthair,
          Russian Blue


          <br><img style="height: 120px;" alt="" src="images/high-pose-variability.jpg">
          <br>Figure 5: Example of high pose variability with four members of Miniature Pinschers
        </div>

      </div>

      <br><br>

      <h4>Motivation</h4>
      Image Classification is one of the most common task in computer vision which are well-studied over past few years. 
      Model architectures have been improved from basic convolutional neural networks to some more complex models such as
      ResNet, Inception, etc. However, majority of these models are tested in common classification tasks such as ImageNet, 
      where most classes expose a relatively low similarity to each other. Therefore, we are interested in whether modern 
      computer vision models have enough power to do classification problems like this. 
      Another motivation is the limitation of data. People usually claim that for complex image classfication problems, 
      at least 1000 images per class are necessary. In our dataset, each breed only has approximately 200 samples, which are
      far more from this widely accepted requirement. We are then interested in how well classifiers can do when images are not
      sufficient and whether data augmentation techniques could improve the performance of our models. 
      <br><br>


      <h4>Backgrounds</h4>
      
      For this dataset specifically, fine-grained recognition works has been demonstrated on cats and dogs by 
      O. M. Parkhi, A. Vedaldi, A. Zisserman and C. V. Jawahar, which trained 63.48% and 55.68% accuracy breed classification 
      for cats and dogs respectively, which improved to 66.07% and 59.18% when the ground truth segmentations are used.
      We are using these models as our baselines. We are looking forward to check if we can build a model from scratch with potential data 
      augmentation techniques to beat them. 

      <br><br>
      <!-- Approach -->
      <h2>Approach</h2>
      <h4>What do you plan to do exactly? How will you solve the problem? Why do you think it will
        be successful? Is anything new in your approach? </h4>
      The project has two main stages: building models and implementing data augmentation.
      First, the first model we want to try is to fine-tuning some famous pre-trained models. We want to see if this approach can give
      any promising results in our task. If so, this fine-tining model is used as our new baseline. 
      Second, we are building models from scratch to see if we can beat baselines(either fine-tuning models or models mentioned in background
      section). Even though there are various large famous architectures in community such as ResNet and GoogleNet, they are too big to be fit
      in our small GPUs. We are trying to build our customized models with some ideas borrowed from those famous architectures but not using their 
      exact models. 
      Once we have such a model, we will include data augmentation in our training process. We'll start with some standard operations and see if 
      performance can be improved. Then, we will implement data augmentation by GAN, which is more complex than simpple image operations. 



      <br><br>
      <!-- Results -->
      <h2>Experimental Plan</h2>
      <h4>How do you plan to measure success? What datasets are you using and what experiments
          will you carry out? </h4>
      Our experimental plan follows our plan of attack. There are three main tasks we need to experiment: dog-cat binary classification, cat breeds classificaion and dog breeds classification. For each of them, our experiments are organized as follows:
      Two main models are used: the model we build from scratch and fine-tuning models. 
      For fine-tuning models, we pick ResNet-50 because of it's promising performance on other tasks and the size of its model. 
      For each model, we also experiment it with different data augmentaiton techniques, namely, no augmentation included, one or two standard data 
      augmentation approach, and one data augmentation technique by GAN. 

      We are curious to see whether we can build a model to beat baselines and how much the model performance can be improved by data augmentation. 

      <br><br>

      <!-- Main Results Figure -->
      <!-- <div style="text-align: center;">
<img style="height: 300px;" alt="" src="images/results.png">
</div>
<br><br>

  <hr>
  <footer> 
  <p>Â© You Name(s) Here</p>
  </footer>

<br><br> -->
      <!-- Analysis -->
      <!-- <h2>Analysis</h2>
<h4>Do the results make sense? Why or why not? Describe what kind of visualization/analysis you performed in order to verify that your results 1) are correct and 2) explain differences in performance from what was expected (e.g. what appeared in papers). Provide specific claims about why you think your model is or is not doing better, and justify those with qualitative and quantitative experiments (not necessarily just final accuracy numbers, but statistics or other data about what the model is doing).</h4>
Nemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. -->

      <!-- Current Status  -->
      <h2>Current Status</h2>
      <!-- Team -->
      <table style="width:100%">
        <tr>
          <th>Name</th>
          <th>Description of Work</th>
        </tr>
        <tr>
          <td>Chongwen Guo</td>
          <td>Data preprocessing and model utils functions (accomplished), train models using Fine-Tune Pretrained ResNet-50 convolutional neural network 
            <br>(accomplished, achived 90%, 55%, 80% accuracy for cat-dog classifier, cat-breed classifier and dog-breed classifier respectively)</td>
        </tr>
        <tr>
          <td>Zhuoran Yu</td>
          <td>build models from scratch (accomplished, achived 60% accarucy for cat-breed classifier), data augmentation with GAN (to do)</td>
        </tr>
      </table>
      <br>
 <!-- Help  -->
 <h2>Help</h2>
 We are on the right track.


      <br><br>
    </div>
  </div>


</body>

</html>
